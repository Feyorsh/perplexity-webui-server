#!/usr/bin/env python3

from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI
from dotenv import load_dotenv, find_dotenv
import uvicorn
import os

from typing import Any, AsyncIterator, Dict, Literal, Sequence
from langchain_openai_api_bridge.core.create_agent_dto import CreateAgentDto
from langchain_openai_api_bridge.assistant import (
    InMemoryMessageRepository,
    InMemoryRunRepository,
    InMemoryThreadRepository,
)
from langchain_openai_api_bridge.fastapi.langchain_openai_api_bridge_fastapi import (
    LangchainOpenaiApiBridgeFastAPI
)
from langchain_core.runnables.config import RunnableConfig
from perplexity_webui_langchain import PerplexityWebUIChatModel
from langchain_core.runnables.schema import StandardStreamEvent, CustomStreamEvent
_ = load_dotenv(find_dotenv())


app = FastAPI(
    title="Langchain Agent OpenAI API Bridge to Perplexity WebUI",
    version="1.0",
    description="OpenAI API exposing langchain agent (Perplexity WebUI) as a RESTful API",

)

app.add_api_route(
    path="/",
    endpoint=lambda: {"message": "Welcome to the Langchain Agent OpenAI API Bridge to Perplexity WebUI"},
    methods=["GET"],
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

in_memory_thread_repository = InMemoryThreadRepository()
in_memory_message_repository = InMemoryMessageRepository()
in_memory_run_repository = InMemoryRunRepository()

class PerplexityWebUIOpenAICompatibleChatModel(PerplexityWebUIChatModel):
    def astream_events(self, input: Any, config: RunnableConfig | None = None, *, version: Literal['v1', 'v2'], 
                             include_names: Sequence[str] | None = None, 
                             include_types: Sequence[str] | None = None, 
                             include_tags: Sequence[str] | None = None, 
                             exclude_names: Sequence[str] | None = None, 
                             exclude_types: Sequence[str] | None = None, 
                             exclude_tags: Sequence[str] | None = None, **kwargs: Any) -> AsyncIterator[StandardStreamEvent | CustomStreamEvent]:
        if type(input) == dict:
            if 'messages' in input:
                input = input['messages']

        return super().astream_events(input=input, config=config, version=version, include_names=include_names, include_types=include_types, include_tags=include_tags, exclude_names=exclude_names, exclude_types=exclude_types, exclude_tags=exclude_tags, **kwargs)

# Keep the chat model alive so we can cache requests
agent = PerplexityWebUIOpenAICompatibleChatModel(email=os.getenv("PERPLEXITY_WEBUI_EMAIL"))
def create_agent(dto: CreateAgentDto):
    return agent

bridge = LangchainOpenaiApiBridgeFastAPI(app=app, agent_factory_provider=create_agent)

bridge.bind_openai_chat_completion(prefix="")
bridge.bind_openai_assistant_api(
    thread_repository_provider=in_memory_thread_repository,
    message_repository_provider=in_memory_message_repository,
    run_repository_provider=in_memory_run_repository,
    prefix="",
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=9000)
